{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Conv3D, Conv3DTranspose, MaxPooling3D, UpSampling3D\n","from tensorflow.keras.models import Model\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.callbacks import TensorBoard\n","from sys import getsizeof\n","np.set_printoptions(threshold=np.inf)\n","import os \n","from keras.optimizers import RMSprop\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":["\n","def Encoder():\n","    inp = tf.keras.Input(shape=(32,256,256,1)) # prima era 64\n","\n","    enc = tf.keras.layers.Conv3D(16, (2,2,2), kernel_initializer='lecun_normal', activation='selu', padding = 'same')(inp)\n","    enc = tf.keras.layers.MaxPooling3D((2,2,2), padding = 'same')(enc)\n"," \n","    enc = tf.keras.layers.Conv3D(32, (2,2,2), kernel_initializer='lecun_normal', activation='selu', padding = 'same')(enc)\n","    enc = tf.keras.layers.MaxPooling3D((2,2,2), padding = 'same')(enc)\n"," \n","    enc = tf.keras.layers.Conv3D(64, (2,2,2), kernel_initializer='lecun_normal', activation='selu', padding = 'same')(enc)\n","    enc = tf.keras.layers.MaxPooling3D((2,2,2), padding = 'same') (enc)\n","\n","    enc = tf.keras.layers.Conv3D(32, (2,2,2), kernel_initializer='lecun_normal', activation='selu', padding = 'same')(enc)\n","    enc = tf.keras.layers.MaxPooling3D((2,2,2), padding = 'same') (enc)\n","\n","    enc = tf.keras.layers.Conv3D(16, (2,2,2), kernel_initializer='lecun_normal', activation='selu', padding = 'same')(enc)\n","    enc = tf.keras.layers.MaxPooling3D((2,2,2), padding = 'same') (enc)\n","\n","    # latentent code vae\n","    latent_code = tf.keras.layers.Flatten()(enc)\n","    latent_code = tf.keras.layers.Dense(64, )(latent_code) #act fun removed.. linear # prima era 256,# prima non c'era sigmoid\n","    \n","    encoder = tf.keras.Model(inp, latent_code, name = 'encoder')\n","    return encoder\n","\n","\n","def Encoder2():\n","    inp = tf.keras.Input(shape=(32,256,256,1)) # prima era 64\n","\n","    enc = tf.keras.layers.Conv3D(16, (2,2,2), padding = 'same')(inp)\n","    enc = tf.keras.layers.LeakyReLU(0.2)(enc)\n","    enc = tf.keras.layers.MaxPooling3D((2,2,2), padding = 'same')(enc)\n"," \n","    enc = tf.keras.layers.Conv3D(32, (2,2,2), padding = 'same')(enc)\n","    #enc = tf.keras.layers.BatchNormalization()(enc)\n","    enc = tf.keras.layers.LeakyReLU(0.2)(enc)\n","    enc = tf.keras.layers.MaxPooling3D((2,2,2), padding = 'same')(enc)\n"," \n","    enc = tf.keras.layers.Conv3D(64, (2,2,2), padding = 'same')(enc)\n","    #enc = tf.keras.layers.BatchNormalization()(enc)\n","    enc = tf.keras.layers.LeakyReLU(0.2)(enc)\n","    enc = tf.keras.layers.MaxPooling3D((2,2,2), padding = 'same') (enc)\n","\n","    enc = tf.keras.layers.Conv3D(32, (2,2,2), padding = 'same')(enc)\n","    #enc = tf.keras.layers.BatchNormalization()(enc)\n","    enc = tf.keras.layers.LeakyReLU(0.2)(enc)\n","    enc = tf.keras.layers.MaxPooling3D((2,2,2), padding = 'same') (enc)\n","\n","    enc = tf.keras.layers.Conv3D(16, (2,2,2), padding = 'same')(enc)\n","    enc = tf.keras.layers.LeakyReLU(0.2)(enc)\n","    enc = tf.keras.layers.MaxPooling3D((2,2,2), padding = 'same') (enc)\n","\n","    # latentent code vae\n","    latent_code = tf.keras.layers.Flatten()(enc)\n","    latent_code = tf.keras.layers.Dense(128, activation = 'sigmoid')(latent_code) #act fun removed.. linear # prima era 256\n","    \n","    encoder = tf.keras.Model(inp, latent_code, name = 'encoder')\n","    return encoder\n","\n","\n","def Decoder():\n","    z = tf.keras.Input(shape=(32,)) # prima era 10 # prima era 256\n","    \n","    # start decoder\n","    rec = tf.keras.layers.Dense(1024, activation='selu')(z) # ripristino le dimensioni complete\n","    rec = tf.keras.layers.Reshape((1, 8, 8, 16))(rec) # riprestinate le dimensioni\n","    \n","    # decoding\n","    code = tf.keras.layers.Conv3DTranspose(16,(2,2,2), strides=(1,1,1), kernel_initializer='lecun_normal', activation='selu', padding = 'same')(rec)\n","    code = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(code)\n","\n","    dec = tf.keras.layers.Conv3DTranspose(32, (2,2,2), strides=(1,1,1), kernel_initializer='lecun_normal', activation='selu', padding='same')(code)\n","    dec = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(dec)\n","    \n","    dec = tf.keras.layers.Conv3DTranspose(64, (2,2,2), strides=(1,1,1), kernel_initializer='lecun_normal', activation='selu', padding='same')(dec)\n","    dec = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(dec)\n","    \n","    dec = tf.keras.layers.Conv3DTranspose(32, (2,2,2), strides=1, kernel_initializer='lecun_normal', activation='selu', padding='same')(dec)\n","    dec = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(dec)\n","    \n","    dec = tf.keras.layers.Conv3DTranspose(16, (2,2,2), strides=1, kernel_initializer='lecun_normal', activation='selu', padding='same')(dec)\n","    dec = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(dec)\n","    \n","    decoded = tf.keras.layers.Conv3D(1, (3,3,3), activation='sigmoid', padding='same')(dec)\n","    #model\n","    decoder = tf.keras.Model(inputs = z, outputs = decoded, name = 'decoder')\n","    return decoder\n","\n","def Decoder2():\n","    z = tf.keras.Input(shape=(128,)) # prima era 10 # prima era 256\n","    \n","    # start decoder\n","    rec = tf.keras.layers.Dense(1024, activation='selu')(z) # ripristino le dimensioni complete\n","    rec = tf.keras.layers.Reshape((1, 8, 8, 16))(rec) # riprestinate le dimensioni\n","    \n","    # decoding\n","    code = tf.keras.layers.Conv3DTranspose(16,(2,2,2), strides=(1,1,1), activation = 'relu', padding = 'same')(rec)\n","    code = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(code)\n","\n","    dec = tf.keras.layers.Conv3DTranspose(32, (2,2,2), strides=(1,1,1), activation='relu', padding='same')(code)\n","    dec = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(dec)\n","    \n","    dec = tf.keras.layers.Conv3DTranspose(64, (2,2,2), strides=(1,1,1), activation='relu', padding='same')(dec)\n","    dec = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(dec)\n","    \n","    dec = tf.keras.layers.Conv3DTranspose(32, (2,2,2), strides=1, activation='relu', padding='same')(dec)\n","    dec = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(dec)\n","    \n","    dec = tf.keras.layers.Conv3DTranspose(16, (2,2,2), strides=1, activation='relu', padding='same')(dec)\n","    dec = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(dec)\n","    \n","    decoded = tf.keras.layers.Conv3D(1, (3,3,3), activation='sigmoid', padding='same')(dec)\n","    #model\n","    decoder = tf.keras.Model(inputs = z, outputs = decoded, name = 'decoder')\n","    return decoder\n","\n","def Discriminator(z_size = 64):# prima era 256\n","    encoded = tf.keras.Input(shape=(z_size,))\n","    \n","    x = tf.keras.layers.Dense(128)(encoded)\n","    x = tf.keras.layers.LeakyReLU(0.2)(x)\n","    x = tf.keras.layers.Dense(128)(x)\n","    x = tf.keras.layers.LeakyReLU(0.2)(x)\n","    \n","    prediction = tf.keras.layers.Dense(1)(x)\n","    model = tf.keras.Model(inputs=encoded, outputs=prediction)\n","    return model\n","\n","\n","class ConvAAE3D(tf.keras.Model):\n","    def __init__(self, encoder, decoder, disc, **kwargs):\n","        super(ConvAAE3D, self).__init__(**kwargs)\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.discriminator = disc\n","        self.loss_weight = 1\n","    \n","    def train_step(self, data):\n","        if isinstance(data, tuple):\n","            data = data[0]\n","        \n","        # enc-dec train\n","        with tf.GradientTape() as tape:\n","            enc = self.encoder(data)\n","            reconstruction = self.decoder(enc)\n","            reconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(data, reconstruction))#prima era binary crossentropy\n","            reconstruction_loss *= 256 * 256\n","            total_loss = reconstruction_loss \n","            \n","        grads = tape.gradient(total_loss, self.encoder.trainable_variables + self.decoder.trainable_variables)\n","        self.optimizer.apply_gradients(zip(grads, self.encoder.trainable_variables + self.decoder.trainable_variables))\n","        \n","        # disc train\n","        with tf.GradientTape() as dc_tape:\n","            real_distribution = tf.random.normal([tf.shape(data)[0], 64], mean=0.0, stddev=1.0) #256 = dimensione spazio latente # prima era 256\n","            \n","            encoder_output = self.encoder(data, training=True)\n","\n","            dc_real = self.discriminator(real_distribution, training=True)\n","            dc_fake = self.discriminator(encoder_output, training=True)\n","\n","            # Discriminator Loss\n","            loss_real = tf.keras.losses.binary_crossentropy(tf.ones_like(dc_real)*0.85, dc_real, from_logits=True) # label smoothing\n","            loss_fake = tf.keras.losses.binary_crossentropy(tf.zeros_like(dc_fake), dc_fake, from_logits=True)\n","            dc_loss = self.loss_weight * tf.reduce_mean(loss_fake + loss_real)\n","\n","        # update gradienti\n","        dc_grads = dc_tape.gradient(dc_loss, self.discriminator.trainable_variables)\n","        self.optimizer.apply_gradients(zip(dc_grads, self.discriminator.trainable_variables))\n","        \n","        with tf.GradientTape() as gen_tape:\n","            encoder_output = self.encoder(data, training=True)\n","            dc_fake = self.discriminator(encoder_output, training=True)\n","\n","            # Generator loss\n","            #gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = dc_fake, labels = tf.ones_like(dc_fake)))\n","            gen_loss = self.loss_weight * tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.ones_like(dc_fake), dc_fake, from_logits=True))\n","        \n","        # update gradienti\n","        gen_grads = gen_tape.gradient(gen_loss, self.encoder.trainable_variables)\n","        self.optimizer.apply_gradients(zip(gen_grads, self.encoder.trainable_variables))\n","            \n","        # metrics\n","        mse = tf.reduce_mean(tf.keras.losses.MSE(data, reconstruction))\n","        mae = tf.reduce_mean(tf.keras.losses.MAE(data, reconstruction))\n","        return {\"rec loss\": total_loss,\n","                \"mse\": mse, \n","                \"mae\": mae, \n","                \"disc loss\": dc_loss, \n","                \"gen_loss\":gen_loss,}\n","    \n","    def test_step(self, data):\n","        if isinstance(data, tuple):\n","            data = data[0]\n","\n","        enc = self.encoder(data)\n","        reconstruction = self.decoder(enc)\n","        reconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(data, reconstruction))\n","        reconstruction_loss *= 256 * 256\n","        total_loss = reconstruction_loss #+ kl_loss\n","        # metrics\n","        mse = tf.reduce_mean(tf.keras.losses.MSE(data, reconstruction))\n","        mae = tf.reduce_mean(tf.keras.losses.MAE(data, reconstruction))\n","        return {\n","            \"rec loss\": total_loss,\n","            \"mse\": mse,\n","            \"mae\": mae,}\n","        \n","        \n","        \n","    def call(self, inputs): # forward pass Implementation\n","        z_mean, z_log_var, z = self.encoder(inputs)\n","        reconstruction = self.decoder(z)\n","        reconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(inputs, reconstruction))\n","        reconstruction_loss *= 256 * 256\n","\n","        total_loss = reconstruction_loss #+ kl_loss\n","        # metrics\n","        mse = tf.reduce_mean(tf.keras.losses.MSE(data, reconstruction))\n","        mae = tf.reduce_mean(tf.keras.losses.MAE(data, reconstruction))\n","        \n","        self.add_metric(total_loss, name='loss', aggregation='mean')\n","        self.add_metric(mse, name='mse', aggregation='mean')\n","        self.add_metric(mae, name='mae', aggregation='mean')\n","        return reconstruction\n","        \n","        \n","\n","def show(imgs,cols, rows):\n","    # Show lungmasked\n","    fig=plt.figure(figsize=(30, 20))\n","    columns = cols\n","    rows = rows\n","\n","    for i in range(1, columns*rows +1):\n","        img = imgs[i-1,:,:]\n","        fig.add_subplot(rows, columns, i)\n","        plt.imshow(img, cmap=\"gray\", vmin = 0, vmax = 1)\n","        plt.title(i, fontsize = 10)\n","        plt.axis('off');\n","    plt.show()\n","Encoder().summary()\n","Encoder2().summary()\n","Decoder().summary()\n","Decoder2().summary()\n","Discriminator().summary()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["#### carico tutte le serie e le normalizzo ####\n","x_train = []\n","for element in os.listdir('../input/resempled-osic'):\n","    if element.endswith('npy'):\n","        x_train.append(np.expand_dims(np.load('../input/resempled-osic/'+element),-1))\n","        \n","\n","x_train = np.array(x_train) #-0.025847689005221518 # mean\n","x_train = np.reshape(x_train, (x_train.shape[0], 32, 256, 256,1))\n","\n","trainx = x_train [0:150]\n","valx = x_train[150:]\n","\n","del x_train\n","\n","print(trainx.shape)\n","print(np.max(trainx),np.min(trainx))\n","print(np.mean(trainx))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["aae = ConvAAE3D(Encoder(), Decoder(), Discriminator())\n","aae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 3e-4 )) #3e-4\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n","    monitor=\"rec loss\",\n","    factor=0.5,\n","    patience=5,\n","    verbose=1,\n","    mode=\"auto\")\n","\n","aae.fit(trainx,\n","        trainx,\n","        epochs = 55,\n","        batch_size = 8,\n","        callbacks=[reduce_lr],\n","        #validation_data=(valx,valx),\n","        verbose = 1)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# learning rate restart x2 times -- intuition based on ccosine decay with lr - restart\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n","    monitor=\"rec loss\",\n","    factor=0.5,\n","    patience=5,\n","    verbose=1,\n","    mode=\"auto\")\n","aae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 3e-4 )) #3e-4\n","aae.fit(trainx,\n","        trainx,\n","        epochs = 65,\n","        batch_size = 8,\n","        callbacks=[reduce_lr],\n","        #validation_data=(valx,valx),\n","        verbose = 1)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["ris = aae.encoder.predict(np.expand_dims(trainx[22,:,:,:],0))\n","code = ris\n","code.shape\n","img = aae.decoder.predict(code)\n","img.shape\n","vol = img[0,:,:,:,:]\n","vol.shape\n","show(vol[:,:,:,0],8, 4)\n","show(trainx[22,:,:,:,0],8, 4)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["aae.encoder.save('./encoderLat32BEST2')\n","aae.decoder.save('./decoderLat32BEST2')\n","aae.discriminator.save('./discLat32BEST2')\n","!zip -r /kaggle/working/convae3Dvedinum4.zip /kaggle/working"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":[],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}